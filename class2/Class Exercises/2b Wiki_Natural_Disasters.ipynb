{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Wikipedia Natural Disasters\n",
    "Follow this step-by-step workflow to collect, clean, and visualise a dataset pulled directly from Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you will learn\n",
    "- Build polite HTTP requests with rotating user agents.\n",
    "- Identify the right tables on a MediaWiki page before scraping.\n",
    "- Convert raw HTML rows into a tidy pandas DataFrame.\n",
    "- Clean numeric ranges, remove footnotes, and fix data types.\n",
    "- Explore the resulting dataset with plots and a simple map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 — Import the libraries\n",
    "We bring in web-scraping helpers, data wrangling tools, and visualisation packages that we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for HTTP requests, HTML parsing, and randomization\n",
    "import requests  # For sending HTTP requests to fetch web pages\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "import random  # For selecting random user agents\n",
    "\n",
    "# Import libraries for data manipulation, regular expressions, and string handling\n",
    "import pandas as pd  # For data manipulation and creating DataFrames\n",
    "import re  # For regular expressions to clean text\n",
    "from io import StringIO  # For reading HTML strings as file-like objects\n",
    "\n",
    "# Import libraries for visualization, mapping, and geocoding\n",
    "import folium  # For creating interactive maps\n",
    "import seaborn as sns  # For statistical data visualization\n",
    "import matplotlib.pyplot as plt  # For plotting graphs\n",
    "from geopy.geocoders import Nominatim, Photon  # For geocoding locations\n",
    "from geopy.extra.rate_limiter import RateLimiter  # For rate-limiting geocoding requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 — Rotate user agents\n",
    "Websites can respond differently depending on the browser profile they think is visiting. Rotating through a small list of realistic user agents helps keep our requests polite and less predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of user agents to rotate through, mimicking different browsers\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/57.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/16.16299',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 — Helper to pick a user agent\n",
    "Each request will call this function so we do not reuse the same header repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/57.0\n"
     ]
    }
   ],
   "source": [
    "# Define a function to randomly select a user agent from the list\n",
    "def get_random_user_agent():\n",
    "    \"\"\"Return a random user-agent header for outgoing HTTP requests.\"\"\"\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "# Print an example of a selected user agent\n",
    "print(f\"Using User-Agent: {get_random_user_agent()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 — Target URL\n",
    "We will scrape the live Wikipedia article so the dataset stays current. Should the structure change, you can re-run the notebook to refresh the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the URL of the Wikipedia page to scrape\n",
    "url = \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 — Download the page and inspect its tables\n",
    "1. Send a request with a random user agent and confirm the status code.\n",
    "2. Parse the HTML with BeautifulSoup so we can navigate the document.\n",
    "3. Collect only the tables that expose the columns `'Year', 'Death Toll', 'Event', 'Countries Affected', 'Type', 'Date'`.\n",
    "4. Preview the first few rows before we tidy the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "✅ XML parsed successfully!\n",
      "✅ Found 34 article titles.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Send HTTP request to the RSS feed with a random User-Agent\n",
    "headers = {\"User-Agent\": get_random_user_agent()}\n",
    "response = requests.get(url, headers=headers)\n",
    "print(\"Status Code:\", response.status_code)\n",
    "\n",
    "# Step 2: Parse the RSS XML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"xml\")\n",
    "print(\"✅ XML parsed successfully!\")\n",
    "\n",
    "# Step 3: Extract all article titles from the feed\n",
    "titles = []\n",
    "for item in soup.find_all(\"item\"):\n",
    "    title = item.title.get_text()\n",
    "    titles.append(title)\n",
    "\n",
    "print(f\"✅ Found {len(titles)} article titles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 — Extract the rows we care about\n",
    "Two collapsible tables share the same structure (years 1900–2000 and 2001–present). We will loop through both, normalise their headers, clean each cell (removing footnotes and non-breaking spaces), and combine the results into a single DataFrame that we also persist to disk for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Display the first 10 article titles\n",
    "print(\"\\nTop 10 Article Titles:\\n\")\n",
    "for i, title in enumerate(titles[:10], start=1):\n",
    "    print(f\"{i}) {title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 — Clean and explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the death toll into comparable integers\n",
    "Many rows record a range such as `6,000–9,000`. We will keep the lower bound so we can sort and chart consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract the lower bound of death toll ranges\n",
    "def extract_lower_bound(death_toll: str) -> int:\n",
    "    \"\"\"Return the first numeric value found in a death toll string.\"\"\"\n",
    "    match = re.search(r\"(\\d+(?:,\\d+)?)\", str(death_toll))  # Search for numbers with optional commas\n",
    "    return int(match.group(1).replace(\",\", \"\")) if match else 0  # Convert to int, remove commas\n",
    "\n",
    "# Apply the function to the Death Toll column\n",
    "df[\"Death Toll\"] = df[\"Death Toll\"].apply(extract_lower_bound)\n",
    "# Display the updated DataFrame head\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the DataFrame shape\n",
    "Confirm the number of rows (disasters) and columns recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display the shape of the DataFrame (rows, columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column names\n",
    "Double-check that our cleaned headers are the ones we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the column names of the DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "A quick numerical overview highlights the distribution of the cleaned death toll values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display descriptive statistics for the DataFrame\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values\n",
    "Missing data may signal parsing issues or follow-up cleaning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and display the number of missing values per column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types\n",
    "Verify each column has the expected Python type before visualising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data types of each column\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforce numeric dtypes\n",
    "Casting to integers makes sure pandas treats years and death tolls as numbers, not strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Year to integer type\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "# Convert Date to datetime type, coercing errors\n",
    "df['Date'] = pd.to_datetime(df['Date'], format=\"%Y-%m-%d\", errors='coerce')\n",
    "# Display updated data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique countries affected\n",
    "How many distinct countries or regions appear in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in 'Countries Affected' column\n",
    "unique_countries_count = df['Countries Affected'].nunique()\n",
    "# Print the count\n",
    "print(f\"{unique_countries_count} unique countries or regions recorded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique disaster types\n",
    "A quick count shows the breadth of disaster categories recorded on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in 'Type' column\n",
    "unique_disaster_types_count = df['Type'].nunique()\n",
    "# Print the count\n",
    "print(f\"{unique_disaster_types_count} distinct disaster types captured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Article Titles:\n",
      "\n",
      "1) Companies Have Shielded Buyers From Tariffs. But Not for Long.\n",
      "2) A Major Crypto Pardon, and the N.B.A. Gambling Scandal With Mob Ties\n",
      "3) Letitia James Case Shows Ruthlessness of Justice Dept. in Trump’s Grip\n",
      "4) Letitia James to Appear in Court as Battle Over Trump-Urged Prosecution Begins\n",
      "5) Rebuilding Israeli-Held Parts of Gaza: Workable or Another U.S. Pipe Dream?\n",
      "6) Who Were the Palestinian Prisoners Freed by Israel?\n",
      "7) N.B.A. Gambling Scandal Reflects America’s Obsession With Sports Betting\n",
      "8) NBA Gambling Scandal: What We Know\n",
      "9) Can Ken Burns Win the American Revolution?\n",
      "10) On a Roll, European Leaders Meet to Bolster Support for Ukraine\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Display the first 10 article titles\n",
    "print(\"\\nTop 10 Article Titles:\\n\")\n",
    "for i, title in enumerate(titles[:10], start=1):\n",
    "    print(f\"{i}) {title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: display the full table\n",
    "Expand pandas' display settings if you want to inspect every column and row inline. Be cautious—large tables can slow down the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 — Visualise the impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year vs. death toll\n",
    "Plotting each disaster as a point shows temporal clusters and highlights extreme events by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "✅ XML parsed successfully!\n",
      "\n",
      "Top 10 Article Titles:\n",
      "\n",
      "1) Companies Have Shielded Buyers From Tariffs. But Not for Long.\n",
      "2) A Major Crypto Pardon, and the N.B.A. Gambling Scandal With Mob Ties\n",
      "3) Letitia James Case Shows Ruthlessness of Justice Dept. in Trump’s Grip\n",
      "4) Letitia James to Appear in Court as Battle Over Trump-Urged Prosecution Begins\n",
      "5) Rebuilding Israeli-Held Parts of Gaza: Workable or Another U.S. Pipe Dream?\n",
      "6) Who Were the Palestinian Prisoners Freed by Israel?\n",
      "7) N.B.A. Gambling Scandal Reflects America’s Obsession With Sports Betting\n",
      "8) NBA Gambling Scandal: What We Know\n",
      "9) Can Ken Burns Win the American Revolution?\n",
      "10) On a Roll, European Leaders Meet to Bolster Support for Ukraine\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Set the RSS feed URL for NYT homepage\n",
    "url = \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\"\n",
    "\n",
    "# Step 2: Send HTTP request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "print(\"Status Code:\", response.status_code)\n",
    "\n",
    "# Step 3: Parse XML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"xml\")\n",
    "print(\"✅ XML parsed successfully!\")\n",
    "\n",
    "# Step 4: Extract article titles from <item> tags\n",
    "titles = [item.title.get_text() for item in soup.find_all(\"item\")]\n",
    "\n",
    "# Step 5: Display the top 10 article titles in numbered format\n",
    "print(\"\\nTop 10 Article Titles:\\n\")\n",
    "for i, title in enumerate(titles[:10], start=1):\n",
    "    print(f\"{i}) {title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total deaths by disaster type\n",
    "Aggregating by type reveals which kinds of disasters have historically been most deadly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "✅ HTML parsed successfully!\n",
      "\n",
      "Top 10 Headlines from NYT Homepage:\n",
      "\n",
      "1) Live\n",
      "2) Top Stories\n",
      "3) Watch Today’s Videos\n",
      "4) More News\n",
      "5) The AthleticSports coverage\n",
      "6) Well\n",
      "7) Culture and Lifestyle\n",
      "8) AudioPodcasts and narrated articles\n",
      "9) GamesDaily puzzles\n",
      "10) Site Index\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Set the NYT homepage URL\n",
    "url = \"https://www.nytimes.com/\"\n",
    "\n",
    "# Step 2: Send request with a User-Agent header\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "print(\"Status Code:\", response.status_code)\n",
    "\n",
    "# Step 3: Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "print(\"✅ HTML parsed successfully!\")\n",
    "\n",
    "# Step 4: Extract article titles (h2 or specific classes)\n",
    "titles = []\n",
    "\n",
    "# NYT articles often have <h2> with 'css-78b01r' or 'esl82me1' classes, check dynamically\n",
    "for h2 in soup.find_all(\"h2\"):\n",
    "    text = h2.get_text(strip=True)\n",
    "    if text:\n",
    "        titles.append(text)\n",
    "\n",
    "# Step 5: Display top 10 headlines\n",
    "print(\"\\nTop 10 Headlines from NYT Homepage:\\n\")\n",
    "for i, title in enumerate(titles[:10], start=1):\n",
    "    print(f\"{i}) {title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total deaths by country or region\n",
    "Summing per location highlights where disasters have had the heaviest tolls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Article Titles:\n",
      "\n",
      "1) Companies Have Shielded Buyers From Tariffs. But Not for Long.\n",
      "2) A Major Crypto Pardon, and the N.B.A. Gambling Scandal With Mob Ties\n",
      "3) Letitia James Case Shows Ruthlessness of Justice Dept. in Trump’s Grip\n",
      "4) Letitia James to Appear in Court as Battle Over Trump-Urged Prosecution Begins\n",
      "5) Rebuilding Israeli-Held Parts of Gaza: Workable or Another U.S. Pipe Dream?\n",
      "6) Who Were the Palestinian Prisoners Freed by Israel?\n",
      "7) N.B.A. Gambling Scandal Reflects America’s Obsession With Sports Betting\n",
      "8) NBA Gambling Scandal: What We Know\n",
      "9) Can Ken Burns Win the American Revolution?\n",
      "10) On a Roll, European Leaders Meet to Bolster Support for Ukraine\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 2: Set the NYT RSS feed URL\n",
    "url = \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\"\n",
    "\n",
    "# Step 3: Fetch the RSS feed\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to fetch the RSS feed\")\n",
    "    exit()\n",
    "\n",
    "# Step 4: Parse XML content\n",
    "soup = BeautifulSoup(response.content, \"xml\")  # parse as XML\n",
    "\n",
    "# Step 5: Extract top 10 article titles\n",
    "items = soup.find_all(\"item\")\n",
    "top_titles = [item.title.text.strip() for item in items[:10]]\n",
    "\n",
    "# Step 6: Display top 10 titles\n",
    "print(\"Top 10 Article Titles:\\n\")\n",
    "for i, title in enumerate(top_titles, 1):\n",
    "    print(f\"{i}) {title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recent 20-year view\n",
    "Filter to the last two decades to surface modern disaster hotspots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "\n",
      "Top 10 Article Titles:\n",
      "\n",
      "1) Companies Have Shielded Buyers From Tariffs. But Not for Long.\n",
      "2) A Major Crypto Pardon, and the N.B.A. Gambling Scandal With Mob Ties\n",
      "3) Letitia James Case Shows Ruthlessness of Justice Dept. in Trump’s Grip\n",
      "4) Letitia James to Appear in Court as Battle Over Trump-Urged Prosecution Begins\n",
      "5) Rebuilding Israeli-Held Parts of Gaza: Workable or Another U.S. Pipe Dream?\n",
      "6) Who Were the Palestinian Prisoners Freed by Israel?\n",
      "7) N.B.A. Gambling Scandal Reflects America’s Obsession With Sports Betting\n",
      "8) NBA Gambling Scandal: What We Know\n",
      "9) The Wider Costs of the N.B.A. Insider-Trading Scandal\n",
      "10) Can Ken Burns Win the American Revolution?\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Set the RSS feed URL\n",
    "url = \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\"\n",
    "\n",
    "# Step 2: Fetch the RSS feed\n",
    "response = requests.get(url)\n",
    "print(\"Status Code:\", response.status_code)\n",
    "\n",
    "# Step 3: Parse the RSS XML\n",
    "soup = BeautifulSoup(response.content, \"xml\")  # Use \"xml\" parser for RSS\n",
    "\n",
    "# Step 4: Extract top 10 article titles\n",
    "items = soup.find_all(\"item\")[:10]  # Get first 10 articles\n",
    "titles = [item.title.text for item in items]\n",
    "\n",
    "# Step 5: Display titles\n",
    "print(\"\\nTop 10 Article Titles:\\n\")\n",
    "for i, title in enumerate(titles, start=1):\n",
    "    print(f\"{i}) {title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaster impact over time\n",
    "Line charts help reveal trajectory trends and make it easier to spot slow-moving crises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot of Year vs Death Toll, colored by Type\n",
    "sns.relplot(\n",
    "    data=df, kind=\"line\",\n",
    "    x=\"Year\", y=\"Death Toll\",\n",
    "    hue=\"Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot excluding common types like Earthquake and Earthquake, Tsunami\n",
    "sns.relplot(\n",
    "    data=df[~df['Type'].isin(['Earthquake', 'Earthquake, Tsunami'])], kind=\"line\",\n",
    "    x=\"Year\", y=\"Death Toll\",\n",
    "    hue=\"Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 deadliest events\n",
    "Sort the dataset to spotlight the most catastrophic disasters on record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 events by Death Toll\n",
    "top_events = df.nlargest(10, 'Death Toll')\n",
    "# Create a horizontal bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Death Toll', y='Event', data=top_events, palette='viridis', hue='Event', legend=False)\n",
    "plt.title('Top 10 Events with Highest Death Toll')\n",
    "plt.xlabel('Death Toll')\n",
    "plt.ylabel('Event')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive map of affected countries\n",
    "Geocode each country and plot markers. This step uses rate-limited lookups (update the user agent with your contact information) and may take a couple of minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a user agent for geocoding services (required by Nominatim)\n",
    "USER_AGENT = \"DisasterGeoMapper/1.0 (contact: your_email@example.com)\"\n",
    "\n",
    "# Create a base folium map centered at (0,0) with zoom level 2\n",
    "m = folium.Map(location=[0, 0], zoom_start=2)\n",
    "\n",
    "# Initialize geocoders with user agent and timeout\n",
    "nominatim = Nominatim(user_agent=USER_AGENT, timeout=10)\n",
    "photon = Photon(user_agent=USER_AGENT, timeout=10)\n",
    "\n",
    "# Apply rate limiting to geocoding functions\n",
    "nominatim_geocode = RateLimiter(nominatim.geocode, min_delay_seconds=1, swallow_exceptions=True)\n",
    "photon_geocode = RateLimiter(photon.geocode, min_delay_seconds=1, swallow_exceptions=True)\n",
    "\n",
    "# Cache for geocoded locations to avoid repeated requests\n",
    "location_cache = {}\n",
    "\n",
    "# Function to geocode a country name with caching and fallback\n",
    "def geocode_country(name: str):\n",
    "    \"\"\"Safely geocode a country name with caching and fallback between Nominatim and Photon.\"\"\"\n",
    "    if name in location_cache:\n",
    "        return location_cache[name]\n",
    "    \n",
    "    location = nominatim_geocode(name)\n",
    "    if location is None:\n",
    "        location = photon_geocode(name)\n",
    "    \n",
    "    location_cache[name] = location\n",
    "    return location\n",
    "\n",
    "# Dictionary to aggregate disasters by country\n",
    "disasters_by_country = {}\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    year = row['Year']\n",
    "    death_toll = row['Death Toll']\n",
    "    countries = row['Countries Affected']\n",
    "    event = row['Event']\n",
    "\n",
    "    # Split countries if multiple are listed\n",
    "    for country in countries.split(','):\n",
    "        country = country.strip()\n",
    "        if not country or country.lower() in {\"various\", \"unknown\"}:\n",
    "            continue\n",
    "        \n",
    "        # Geocode the country\n",
    "        location = geocode_country(country)\n",
    "        if location is None:\n",
    "            continue\n",
    "        \n",
    "        latitude, longitude = location.latitude, location.longitude\n",
    "        # Create tooltip with disaster details\n",
    "        tooltip = f\"Year: {year}<br>Death Toll: {death_toll}<br>Country: {country}<br>Event: {event}\"\n",
    "        # Append to list for this country\n",
    "        disasters_by_country.setdefault(country, []).append((latitude, longitude, tooltip))\n",
    "\n",
    "# Add markers to the map, averaging locations per country\n",
    "for country, disasters in disasters_by_country.items():\n",
    "    country_latitude = sum(lat for lat, lon, _ in disasters) / len(disasters)\n",
    "    country_longitude = sum(lon for lat, lon, _ in disasters) / len(disasters)\n",
    "    country_tooltip = \"<br>\".join(tooltip for _, _, tooltip in disasters)\n",
    "    folium.Marker([country_latitude, country_longitude], tooltip=country_tooltip).add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the interactive map in the notebook\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the map to an HTML file\n",
    "m.save('impact_by_country_map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
