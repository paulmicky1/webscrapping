{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Wikipedia Natural Disasters\n",
    "Follow this step-by-step workflow to collect, clean, and visualise a dataset pulled directly from Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you will learn\n",
    "- Build polite HTTP requests with rotating user agents.\n",
    "- Identify the right tables on a MediaWiki page before scraping.\n",
    "- Convert raw HTML rows into a tidy pandas DataFrame.\n",
    "- Clean numeric ranges, remove footnotes, and fix data types.\n",
    "- Explore the resulting dataset with plots and a simple map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 — Import the libraries\n",
    "We bring in web-scraping helpers, data wrangling tools, and visualisation packages that we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for HTTP requests, HTML parsing, and randomization\n",
    "import requests  # For sending HTTP requests to fetch web pages\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "import random  # For selecting random user agents\n",
    "\n",
    "# Import libraries for data manipulation, regular expressions, and string handling\n",
    "import pandas as pd  # For data manipulation and creating DataFrames\n",
    "import re  # For regular expressions to clean text\n",
    "from io import StringIO  # For reading HTML strings as file-like objects\n",
    "\n",
    "# Import libraries for visualization, mapping, and geocoding\n",
    "import folium  # For creating interactive maps\n",
    "import seaborn as sns  # For statistical data visualization\n",
    "import matplotlib.pyplot as plt  # For plotting graphs\n",
    "from geopy.geocoders import Nominatim, Photon  # For geocoding locations\n",
    "from geopy.extra.rate_limiter import RateLimiter  # For rate-limiting geocoding requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 — Rotate user agents\n",
    "Websites can respond differently depending on the browser profile they think is visiting. Rotating through a small list of realistic user agents helps keep our requests polite and less predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of user agents to rotate through, mimicking different browsers\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/57.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/16.16299',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 — Helper to pick a user agent\n",
    "Each request will call this function so we do not reuse the same header repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to randomly select a user agent from the list\n",
    "def get_random_user_agent():\n",
    "    \"\"\"Return a random user-agent header for outgoing HTTP requests.\"\"\"\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "# Print an example of a selected user agent\n",
    "print(f\"Using User-Agent: {get_random_user_agent()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 — Target URL\n",
    "We will scrape the live Wikipedia article so the dataset stays current. Should the structure change, you can re-run the notebook to refresh the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the URL of the Wikipedia page to scrape\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_natural_disasters_by_death_toll\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 — Download the page and inspect its tables\n",
    "1. Send a request with a random user agent and confirm the status code.\n",
    "2. Parse the HTML with BeautifulSoup so we can navigate the document.\n",
    "3. Collect only the tables that expose the columns `'Year', 'Death Toll', 'Event', 'Countries Affected', 'Type', 'Date'`.\n",
    "4. Preview the first few rows before we tidy the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to make an HTTP GET request to the URL\n",
    "try:\n",
    "    # Send the request with a random user agent and set a timeout\n",
    "    response = requests.get(url, headers={\"User-Agent\": get_random_user_agent()}, timeout=30)\n",
    "    # Raise an exception if the response status is not successful\n",
    "    response.raise_for_status()\n",
    "except requests.RequestException as exc:\n",
    "    # Exit the program if there's a request error\n",
    "    raise SystemExit(f\"Request error: {exc}\") from exc\n",
    "\n",
    "# Print the HTTP status code and the size of the downloaded content\n",
    "print(f\"HTTP status: {response.status_code}\")\n",
    "print(f\"Downloaded {len(response.content):,} bytes of HTML\")\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all tables with the specific classes for collapsible sortable tables\n",
    "candidate_tables = soup.select(\"table.wikitable.sortable.mw-collapsible\")\n",
    "# Alternative way: candidate_tables = soup.find_all(\"table\", class_=\"wikitable sortable mw-collapsible\")\n",
    "\n",
    "# Check if any tables were found, otherwise exit\n",
    "if not candidate_tables:\n",
    "    raise SystemExit(\"No collapsible wikitable tables found. Check if the page layout has changed.\")\n",
    "\n",
    "# Print the number of candidate tables found\n",
    "print(f\"Found {len(candidate_tables)} collapsible sortable tables on the page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 — Extract the rows we care about\n",
    "Two collapsible tables share the same structure (years 1900–2000 and 2001–present). We will loop through both, normalise their headers, clean each cell (removing footnotes and non-breaking spaces), and combine the results into a single DataFrame that we also persist to disk for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the required columns for the dataset\n",
    "REQUIRED_COLUMNS = [\"Year\", \"Death Toll\", \"Event\", \"Countries Affected\", \"Type\", \"Date\"]\n",
    "\n",
    "# Dictionary to map various column name variations to standardized names\n",
    "COLUMN_ALIASES = {\n",
    "    \"death toll\": \"Death Toll\",\n",
    "    \"death tolls\": \"Death Toll\",\n",
    "    \"deaths\": \"Death Toll\",\n",
    "    \"estimated deaths\": \"Death Toll\",\n",
    "    \"total deaths\": \"Death Toll\",\n",
    "    \"event\": \"Event\",\n",
    "    \"event name\": \"Event\",\n",
    "    \"disaster\": \"Event\",\n",
    "    \"natural disaster\": \"Event\",\n",
    "    \"incident\": \"Event\",\n",
    "    \"countries affected\": \"Countries Affected\",\n",
    "    \"country\": \"Countries Affected\",\n",
    "    \"country or region\": \"Countries Affected\",\n",
    "    \"countries\": \"Countries Affected\",\n",
    "    \"countries or regions\": \"Countries Affected\",\n",
    "    \"countries and regions\": \"Countries Affected\",\n",
    "    \"location\": \"Countries Affected\",\n",
    "    \"locations\": \"Countries Affected\",\n",
    "    \"area affected\": \"Countries Affected\",\n",
    "    \"region\": \"Countries Affected\",\n",
    "    \"regions\": \"Countries Affected\",\n",
    "    \"type\": \"Type\",\n",
    "    \"types\": \"Type\",\n",
    "    \"disaster type\": \"Type\",\n",
    "    \"type of disaster\": \"Type\",\n",
    "    \"natural disaster type\": \"Type\",\n",
    "    \"date\": \"Date\",\n",
    "    \"dates\": \"Date\",\n",
    "    \"year\": \"Year\",\n",
    "    \"years\": \"Year\",\n",
    "}\n",
    "\n",
    "# Compile regular expressions for cleaning text\n",
    "CITATION_PATTERN = re.compile(r\"\\[[^\\]]+\\]\")  # Pattern to remove citations like [1]\n",
    "WHITESPACE_PATTERN = re.compile(r\"\\s+\")  # Pattern to normalize whitespace\n",
    "\n",
    "# Function to clean strings by removing citations and normalizing whitespace\n",
    "def clean_string(value):\n",
    "    \"\"\"Return a citation-free, whitespace-normalised string.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "    text = str(value)\n",
    "    text = CITATION_PATTERN.sub(\"\", text)  # Remove citations\n",
    "    text = text.replace(\"\\xa0\", \" \")  # Replace non-breaking spaces\n",
    "    text = text.strip()  # Strip leading/trailing whitespace\n",
    "    return WHITESPACE_PATTERN.sub(\" \", text)  # Normalize internal whitespace\n",
    "\n",
    "# Function to standardize column names based on aliases and patterns\n",
    "def standardise_column(label):\n",
    "    if isinstance(label, tuple):\n",
    "        label = \" \".join(str(part) for part in label if part and not pd.isna(part))\n",
    "    cleaned = clean_string(label)\n",
    "    slug = re.sub(r\"[^a-z0-9]+\", \" \", cleaned.lower()).strip()\n",
    "    if slug in COLUMN_ALIASES:\n",
    "        return COLUMN_ALIASES[slug]\n",
    "    if \"death\" in slug and (\"toll\" in slug or \"deaths\" in slug):\n",
    "        return \"Death Toll\"\n",
    "    if \"event\" in slug or \"disaster\" in slug or \"incident\" in slug:\n",
    "        return \"Event\"\n",
    "    if \"type\" in slug:\n",
    "        return \"Type\"\n",
    "    if \"country\" in slug or \"location\" in slug or \"region\" in slug or \"area\" in slug:\n",
    "        return \"Countries Affected\"\n",
    "    if \"date\" in slug:\n",
    "        return \"Date\"\n",
    "    if \"year\" in slug:\n",
    "        return \"Year\"\n",
    "    return cleaned\n",
    "\n",
    "# List to hold DataFrames from each table\n",
    "tables_dfs = []\n",
    "matched_tables = 0\n",
    "\n",
    "# Loop through each candidate table\n",
    "for table in candidate_tables:\n",
    "    # Parse the table into a DataFrame using pandas\n",
    "    parsed_frames = pd.read_html(StringIO(str(table)), flavor=\"bs4\", header=0)\n",
    "    if not parsed_frames:\n",
    "        continue\n",
    "    table_df = parsed_frames[0]\n",
    "    # Standardize column names\n",
    "    table_df.columns = [standardise_column(col) for col in table_df.columns]\n",
    "    table_df = table_df.loc[:, ~table_df.columns.duplicated()]  # Remove duplicate columns\n",
    "\n",
    "    # Handle missing columns by inferring or defaulting\n",
    "    if \"Year\" not in table_df.columns and \"Date\" in table_df.columns:\n",
    "        table_df[\"Year\"] = table_df[\"Date\"]\n",
    "    if \"Date\" not in table_df.columns and \"Year\" in table_df.columns:\n",
    "        table_df[\"Date\"] = table_df[\"Year\"]\n",
    "    if \"Type\" not in table_df.columns:\n",
    "        table_df[\"Type\"] = \"Unknown\"\n",
    "    if \"Countries Affected\" not in table_df.columns:\n",
    "        table_df[\"Countries Affected\"] = \"Unknown\"\n",
    "\n",
    "    # Check if all required columns are present\n",
    "    required_missing = set(REQUIRED_COLUMNS) - set(table_df.columns)\n",
    "    if required_missing:\n",
    "        continue\n",
    "\n",
    "    # Select only the required columns\n",
    "    table_df = table_df[REQUIRED_COLUMNS].copy()\n",
    "    # Clean all cells in the DataFrame\n",
    "    table_df = table_df.apply(lambda x: x.apply(clean_string)).replace(\"\", pd.NA)\n",
    "    # Forward fill missing values for certain columns\n",
    "    table_df[\"Countries Affected\"] = table_df[\"Countries Affected\"].ffill().fillna(\"Unknown\")\n",
    "    table_df[\"Type\"] = table_df[\"Type\"].ffill().fillna(\"Unknown\")\n",
    "    # Remove header rows that might have been parsed as data\n",
    "    header_mask = table_df[\"Event\"].fillna(\"\").str.lower().eq(\"event\")\n",
    "    table_df = table_df[~header_mask]\n",
    "\n",
    "    if table_df.empty:\n",
    "        continue\n",
    "\n",
    "    tables_dfs.append(table_df)\n",
    "    matched_tables += 1\n",
    "\n",
    "# If no tables were processed, raise an error\n",
    "if not tables_dfs:\n",
    "    raise ValueError(\n",
    "        \"No rows extracted after header normalisation. Inspect the page to confirm the expected columns are still present.\"\n",
    "    )\n",
    "\n",
    "# Concatenate all table DataFrames into one\n",
    "df = pd.concat(tables_dfs, ignore_index=True)\n",
    "# Extract year as integer from strings\n",
    "df[\"Year\"] = df[\"Year\"].astype(str).str.extract(r\"(\\d{3,4})\")\n",
    "# Drop rows with missing Year or Event\n",
    "df = df.dropna(subset=[\"Year\", \"Event\"]).reset_index(drop=True)\n",
    "\n",
    "# Print summary of extraction\n",
    "print(f\"Matched {matched_tables} tables and collected {len(df):,} rows\")\n",
    "\n",
    "# Save the raw DataFrame to CSV\n",
    "df.to_csv(\"natural_disasters.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Saved 'natural_disasters.csv' with the raw scrape.\")\n",
    "# Display the first few rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 — Clean and explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the death toll into comparable integers\n",
    "Many rows record a range such as `6,000–9,000`. We will keep the lower bound so we can sort and chart consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract the lower bound of death toll ranges\n",
    "def extract_lower_bound(death_toll: str) -> int:\n",
    "    \"\"\"Return the first numeric value found in a death toll string.\"\"\"\n",
    "    match = re.search(r\"(\\d+(?:,\\d+)?)\", str(death_toll))  # Search for numbers with optional commas\n",
    "    return int(match.group(1).replace(\",\", \"\")) if match else 0  # Convert to int, remove commas\n",
    "\n",
    "# Apply the function to the Death Toll column\n",
    "df[\"Death Toll\"] = df[\"Death Toll\"].apply(extract_lower_bound)\n",
    "# Display the updated DataFrame head\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the DataFrame shape\n",
    "Confirm the number of rows (disasters) and columns recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display the shape of the DataFrame (rows, columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column names\n",
    "Double-check that our cleaned headers are the ones we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the column names of the DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "A quick numerical overview highlights the distribution of the cleaned death toll values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display descriptive statistics for the DataFrame\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values\n",
    "Missing data may signal parsing issues or follow-up cleaning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and display the number of missing values per column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types\n",
    "Verify each column has the expected Python type before visualising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data types of each column\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforce numeric dtypes\n",
    "Casting to integers makes sure pandas treats years and death tolls as numbers, not strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Year to integer type\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "# Convert Date to datetime type, coercing errors\n",
    "df['Date'] = pd.to_datetime(df['Date'], format=\"%Y-%m-%d\", errors='coerce')\n",
    "# Display updated data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique countries affected\n",
    "How many distinct countries or regions appear in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in 'Countries Affected' column\n",
    "unique_countries_count = df['Countries Affected'].nunique()\n",
    "# Print the count\n",
    "print(f\"{unique_countries_count} unique countries or regions recorded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique disaster types\n",
    "A quick count shows the breadth of disaster categories recorded on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in 'Type' column\n",
    "unique_disaster_types_count = df['Type'].nunique()\n",
    "# Print the count\n",
    "print(f\"{unique_disaster_types_count} distinct disaster types captured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show full table (optional, can be slow for large data)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display the full DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: display the full table\n",
    "Expand pandas' display settings if you want to inspect every column and row inline. Be cautious—large tables can slow down the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 — Visualise the impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year vs. death toll\n",
    "Plotting each disaster as a point shows temporal clusters and highlights extreme events by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of Year vs Death Toll, colored by Type\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='Year', y='Death Toll', data=df, hue='Type', style='Type', palette='Set1', s=100)\n",
    "plt.title('Scatter Plot of Year vs. Death Toll by Type')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Death Toll')\n",
    "plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total deaths by disaster type\n",
    "Aggregating by type reveals which kinds of disasters have historically been most deadly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Type and sum Death Toll, then sort descending\n",
    "total_death_by_type = df.groupby('Type')['Death Toll'].sum().reset_index().sort_values('Death Toll', ascending=False)\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Type', y='Death Toll', data=total_death_by_type, palette='Set1', hue='Type', legend=False)\n",
    "plt.title('Total Death Toll by Type')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Total Death Toll')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total deaths by country or region\n",
    "Summing per location highlights where disasters have had the heaviest tolls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Countries Affected and sum Death Toll, then sort descending\n",
    "total_death_by_type = df.groupby('Countries Affected')['Death Toll'].sum().reset_index().sort_values('Death Toll', ascending=False)\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Countries Affected', y='Death Toll', data=total_death_by_type, palette='Set1', hue='Countries Affected', legend=False)\n",
    "plt.title('Total Death Toll by Countries affected')\n",
    "plt.xlabel('Countries affected')\n",
    "plt.ylabel('Total Death Toll')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recent 20-year view\n",
    "Filter to the last two decades to surface modern disaster hotspots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DataFrame to years after 2003, group by Countries Affected, sum Death Toll, sort descending\n",
    "total_death_by_type = df[df['Year']>2003].groupby('Countries Affected')['Death Toll'].sum().reset_index().sort_values('Death Toll', ascending=False)\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Countries Affected', y='Death Toll', data=total_death_by_type, palette='Set1', hue='Countries Affected', legend=False)\n",
    "plt.title('Total Death Toll by Countries affected')\n",
    "plt.xlabel('Countries affected')\n",
    "plt.ylabel('Total Death Toll')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaster impact over time\n",
    "Line charts help reveal trajectory trends and make it easier to spot slow-moving crises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot of Year vs Death Toll, colored by Type\n",
    "sns.relplot(\n",
    "    data=df, kind=\"line\",\n",
    "    x=\"Year\", y=\"Death Toll\",\n",
    "    hue=\"Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot excluding common types like Earthquake and Earthquake, Tsunami\n",
    "sns.relplot(\n",
    "    data=df[~df['Type'].isin(['Earthquake', 'Earthquake, Tsunami'])], kind=\"line\",\n",
    "    x=\"Year\", y=\"Death Toll\",\n",
    "    hue=\"Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 deadliest events\n",
    "Sort the dataset to spotlight the most catastrophic disasters on record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 events by Death Toll\n",
    "top_events = df.nlargest(10, 'Death Toll')\n",
    "# Create a horizontal bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Death Toll', y='Event', data=top_events, palette='viridis', hue='Event', legend=False)\n",
    "plt.title('Top 10 Events with Highest Death Toll')\n",
    "plt.xlabel('Death Toll')\n",
    "plt.ylabel('Event')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive map of affected countries\n",
    "Geocode each country and plot markers. This step uses rate-limited lookups (update the user agent with your contact information) and may take a couple of minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a user agent for geocoding services (required by Nominatim)\n",
    "USER_AGENT = \"DisasterGeoMapper/1.0 (contact: your_email@example.com)\"\n",
    "\n",
    "# Create a base folium map centered at (0,0) with zoom level 2\n",
    "m = folium.Map(location=[0, 0], zoom_start=2)\n",
    "\n",
    "# Initialize geocoders with user agent and timeout\n",
    "nominatim = Nominatim(user_agent=USER_AGENT, timeout=10)\n",
    "photon = Photon(user_agent=USER_AGENT, timeout=10)\n",
    "\n",
    "# Apply rate limiting to geocoding functions\n",
    "nominatim_geocode = RateLimiter(nominatim.geocode, min_delay_seconds=1, swallow_exceptions=True)\n",
    "photon_geocode = RateLimiter(photon.geocode, min_delay_seconds=1, swallow_exceptions=True)\n",
    "\n",
    "# Cache for geocoded locations to avoid repeated requests\n",
    "location_cache = {}\n",
    "\n",
    "# Function to geocode a country name with caching and fallback\n",
    "def geocode_country(name: str):\n",
    "    \"\"\"Safely geocode a country name with caching and fallback between Nominatim and Photon.\"\"\"\n",
    "    if name in location_cache:\n",
    "        return location_cache[name]\n",
    "    \n",
    "    location = nominatim_geocode(name)\n",
    "    if location is None:\n",
    "        location = photon_geocode(name)\n",
    "    \n",
    "    location_cache[name] = location\n",
    "    return location\n",
    "\n",
    "# Dictionary to aggregate disasters by country\n",
    "disasters_by_country = {}\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    year = row['Year']\n",
    "    death_toll = row['Death Toll']\n",
    "    countries = row['Countries Affected']\n",
    "    event = row['Event']\n",
    "\n",
    "    # Split countries if multiple are listed\n",
    "    for country in countries.split(','):\n",
    "        country = country.strip()\n",
    "        if not country or country.lower() in {\"various\", \"unknown\"}:\n",
    "            continue\n",
    "        \n",
    "        # Geocode the country\n",
    "        location = geocode_country(country)\n",
    "        if location is None:\n",
    "            continue\n",
    "        \n",
    "        latitude, longitude = location.latitude, location.longitude\n",
    "        # Create tooltip with disaster details\n",
    "        tooltip = f\"Year: {year}<br>Death Toll: {death_toll}<br>Country: {country}<br>Event: {event}\"\n",
    "        # Append to list for this country\n",
    "        disasters_by_country.setdefault(country, []).append((latitude, longitude, tooltip))\n",
    "\n",
    "# Add markers to the map, averaging locations per country\n",
    "for country, disasters in disasters_by_country.items():\n",
    "    country_latitude = sum(lat for lat, lon, _ in disasters) / len(disasters)\n",
    "    country_longitude = sum(lon for lat, lon, _ in disasters) / len(disasters)\n",
    "    country_tooltip = \"<br>\".join(tooltip for _, _, tooltip in disasters)\n",
    "    folium.Marker([country_latitude, country_longitude], tooltip=country_tooltip).add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the interactive map in the notebook\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the map to an HTML file\n",
    "m.save('impact_by_country_map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
